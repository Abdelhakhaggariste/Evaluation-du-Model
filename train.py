import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Importations
from env_iot_mec import IoTMECEnv
from models import ActorCritic
from ppo_agent import PPOAgent

# Configuration
N_IOT = 10
STATE_DIM = 6  # Nombre de caract√©ristiques par IoT
INPUT_DIM = N_IOT * STATE_DIM  # Dimension totale de l'entr√©e
ACTION_DIM = 2  # 0=local, 1=MEC
EPISODES = 200
STEPS_PER_EPISODE = 20

print("="*60)
print("ENTRA√éNEMENT DRL POUR IoT-MEC")
print("="*60)
print(f"Configuration:")
print(f"  ‚Ä¢ IoT: {N_IOT}")
print(f"  ‚Ä¢ √âtat par IoT: {STATE_DIM} features")
print(f"  ‚Ä¢ Dimension entr√©e: {INPUT_DIM}")
print(f"  ‚Ä¢ Actions: {ACTION_DIM} (0=local, 1=MEC)")
print(f"  ‚Ä¢ √âpisodes: {EPISODES}")
print(f"  ‚Ä¢ Steps par √©pisode: {STEPS_PER_EPISODE}")
print("="*60)

# Initialisation
print("\nInitialisation de l'environnement...")
env = IoTMECEnv(n_iot=N_IOT)
print(f"  Observation space: {env.observation_space.shape}")
print(f"  Action space: {env.action_space.nvec}")

print("\nInitialisation du mod√®le...")
model = ActorCritic(
    input_dim=INPUT_DIM,
    action_dim=ACTION_DIM,
    n_iot=N_IOT
)
print(f"  Mod√®le cr√©√© avec succ√®s")
print(f"  Param√®tres totaux: {sum(p.numel() for p in model.parameters()):,}")

print("\nInitialisation de l'agent PPO...")
agent = PPOAgent(
    model=model,
    n_iot=N_IOT,
    lr=3e-4,
    gamma=0.99,
    eps_clip=0.2
)

# Historique des m√©triques
metrics_history = defaultdict(list)

print("\n" + "="*60)
print("D√âBUT DE L'ENTRA√éNEMENT")
print("="*60)

for episode in range(EPISODES):
    # R√©initialiser l'environnement
    state, _ = env.reset()
    episode_rewards = []
    episode_energies = []
    episode_latencies = []
    episode_successes = []
    episode_offloads = []
    
    for step in range(STEPS_PER_EPISODE):
        # S√©lectionner les actions
        actions, log_probs, value = agent.select_action(state)
        
        # Ex√©cuter les actions
        next_state, reward, terminated, truncated, info = env.step(actions.numpy())
        
        # Stocker la transition
        agent.store_transition(state, actions, log_probs, value, reward, terminated)
        
        # Collecter les m√©triques
        episode_rewards.append(reward)
        episode_energies.append(info['energy'])
        episode_latencies.append(info['latency'])
        episode_successes.append(info['success_rate'])
        episode_offloads.append(info['offload_rate'])
        
        # Mettre √† jour l'√©tat
        state = next_state
        
        # Si c'est la fin de l'√©pisode, mettre √† jour l'agent
        if terminated or step == STEPS_PER_EPISODE - 1:
            loss = agent.update()
            break
    
    # Calculer les moyennes pour l'√©pisode
    avg_reward = np.mean(episode_rewards) if episode_rewards else 0
    avg_energy = np.mean(episode_energies) if episode_energies else 0
    avg_latency = np.mean(episode_latencies) if episode_latencies else 0
    avg_success = np.mean(episode_successes) if episode_successes else 0
    avg_offload = np.mean(episode_offloads) if episode_offloads else 0
    
    # Stocker dans l'historique
    metrics_history['episode'].append(episode)
    metrics_history['total_reward'].append(avg_reward)
    metrics_history['avg_energy'].append(avg_energy)
    metrics_history['avg_latency'].append(avg_latency)
    metrics_history['success_rate'].append(avg_success)
    metrics_history['offload_rate'].append(avg_offload)
    metrics_history['loss'].append(loss if 'loss' in locals() else 0)
    
    # Afficher les progr√®s
    if (episode + 1) % 20 == 0 or episode == 0:
        print(f"√âpisode {episode + 1:3d}/{EPISODES} | "
              f"R√©compense: {avg_reward:7.4f} | "
              f"√ânergie: {avg_energy:6.3f} J | "
              f"Latence: {avg_latency:6.1f} ms | "
              f"Succ√®s: {avg_success:6.2%} | "
              f"D√©chargement: {avg_offload:6.2%}")

print("\n" + "="*60)
print("ENTRA√éNEMENT TERMIN√â")
print("="*60)

# Sauvegarde des m√©triques
print("\nSauvegarde des m√©triques...")
df_metrics = pd.DataFrame(metrics_history)
df_metrics.to_csv('training_metrics.csv', index=False)
print(f"  M√©triques sauvegard√©es dans 'training_metrics.csv'")

# Sauvegarde du mod√®le
print("\nSauvegarde du mod√®le...")
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': agent.optimizer.state_dict(),
    'metrics': metrics_history
}, 'ppo_model.pth')
print(f"  Mod√®le sauvegard√© dans 'ppo_model.pth'")

# 1. VISUALISATION PRINCIPALE
print("\nG√©n√©ration des visualisations...")
plt.figure(figsize=(16, 10))

# 1.1 R√©compense
plt.subplot(2, 3, 1)
plt.plot(metrics_history['episode'], metrics_history['total_reward'], 
         'b-', linewidth=2, alpha=0.8)
plt.fill_between(metrics_history['episode'], 
                 np.array(metrics_history['total_reward']) * 0.95,
                 np.array(metrics_history['total_reward']) * 1.05,
                 alpha=0.2, color='blue')
plt.xlabel('√âpisodes')
plt.ylabel('R√©compense Moyenne')
plt.title('√âvolution de la R√©compense')
plt.grid(True, alpha=0.3)

# 1.2 √ânergie
plt.subplot(2, 3, 2)
plt.plot(metrics_history['episode'], metrics_history['avg_energy'], 
         'r-', linewidth=2, alpha=0.8)
plt.xlabel('√âpisodes')
plt.ylabel('√ânergie Moyenne (J)')
plt.title('Consommation √ânerg√©tique')
plt.grid(True, alpha=0.3)

# 1.3 Latence
plt.subplot(2, 3, 3)
plt.plot(metrics_history['episode'], metrics_history['avg_latency'], 
         'g-', linewidth=2, alpha=0.8)
plt.xlabel('√âpisodes')
plt.ylabel('Latence Moyenne (ms)')
plt.title('Performance de Latence')
plt.grid(True, alpha=0.3)

# 1.4 Taux de succ√®s
plt.subplot(2, 3, 4)
plt.plot(metrics_history['episode'], metrics_history['success_rate'], 
         'c-', linewidth=2, alpha=0.8)
plt.xlabel('√âpisodes')
plt.ylabel('Taux de Succ√®s')
plt.title('Fiabilit√© des T√¢ches')
plt.grid(True, alpha=0.3)

# 1.5 Taux de d√©chargement
plt.subplot(2, 3, 5)
plt.plot(metrics_history['episode'], metrics_history['offload_rate'], 
         'm-', linewidth=2, alpha=0.8)
plt.xlabel('√âpisodes')
plt.ylabel('Taux de D√©chargement')
plt.title('Strat√©gie de D√©chargement')
plt.grid(True, alpha=0.3)

# 1.6 Loss
plt.subplot(2, 3, 6)
plt.plot(metrics_history['episode'], metrics_history['loss'], 
         'k-', linewidth=2, alpha=0.8)
plt.xlabel('√âpisodes')
plt.ylabel('Valeur de Loss')
plt.title('Convergence de l\'Apprentissage')
plt.grid(True, alpha=0.3)

plt.suptitle('Analyse Compl√®te des Performances DRL IoT-MEC', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('multi_metrics_analysis.png', dpi=300, bbox_inches='tight')
print("  ‚úì multi_metrics_analysis.png")

# 2. VISUALISATION COMBIN√âE
plt.figure(figsize=(14, 8))

# Normaliser les m√©triques
episodes = metrics_history['episode']
reward_norm = (np.array(metrics_history['total_reward']) - np.min(metrics_history['total_reward'])) / \
              (np.max(metrics_history['total_reward']) - np.min(metrics_history['total_reward']))
energy_norm = 1 - ((np.array(metrics_history['avg_energy']) - np.min(metrics_history['avg_energy'])) / \
                  (np.max(metrics_history['avg_energy']) - np.min(metrics_history['avg_energy'])))
latency_norm = 1 - ((np.array(metrics_history['avg_latency']) - np.min(metrics_history['avg_latency'])) / \
                   (np.max(metrics_history['avg_latency']) - np.min(metrics_history['avg_latency'])))

# Toutes les m√©triques sur un m√™me graphique
plt.plot(episodes, reward_norm, 'b-', linewidth=2, label='R√©compense (normalis√©e)')
plt.plot(episodes, energy_norm, 'r-', linewidth=2, label='√ânergie (invers√©e)')
plt.plot(episodes, latency_norm, 'g-', linewidth=2, label='Latence (invers√©e)')
plt.plot(episodes, metrics_history['success_rate'], 'c-', linewidth=2, label='Taux de succ√®s')
plt.plot(episodes, metrics_history['offload_rate'], 'm-', linewidth=2, label='Taux de d√©chargement')

plt.xlabel('√âpisodes d\'Apprentissage')
plt.ylabel('Valeur Normalis√©e')
plt.title('√âvolution Conjointe de Toutes les M√©triques IoT-MEC', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', ncol=2)
plt.grid(True, alpha=0.3)

# Zones d'apprentissage
plt.axvspan(0, 50, alpha=0.1, color='red', label='Phase d\'exploration')
plt.axvspan(50, 150, alpha=0.1, color='yellow', label='Phase d\'apprentissage')
plt.axvspan(150, 200, alpha=0.1, color='green', label='Phase de convergence')

plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1))
plt.tight_layout()
plt.savefig('combined_metrics_evolution.png', dpi=300, bbox_inches='tight')
print("  ‚úì combined_metrics_evolution.png")

# 3. VISUALISATION AVANC√âE - COMPARAISON
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 3.1 √ânergie vs Latence (trade-off)
scatter = axes[0, 0].scatter(metrics_history['avg_energy'], 
                             metrics_history['avg_latency'],
                             c=metrics_history['episode'], 
                             cmap='viridis', 
                             alpha=0.7,
                             s=50)
axes[0, 0].set_xlabel('√ânergie (J)')
axes[0, 0].set_ylabel('Latence (ms)')
axes[0, 0].set_title('Compromis √ânergie-Latence')
axes[0, 0].grid(True, alpha=0.3)
plt.colorbar(scatter, ax=axes[0, 0], label='√âpisode')

# 3.2 R√©compense vs D√©chargement
axes[0, 1].scatter(metrics_history['offload_rate'], 
                   metrics_history['total_reward'],
                   c=metrics_history['episode'],
                   cmap='plasma',
                   alpha=0.7,
                   s=50)
axes[0, 1].set_xlabel('Taux de D√©chargement')
axes[0, 1].set_ylabel('R√©compense')
axes[0, 1].set_title('Impact du D√©chargement sur la Performance')
axes[0, 1].grid(True, alpha=0.3)

# 3.3 Succ√®s vs √ânergie
axes[1, 0].scatter(metrics_history['avg_energy'], 
                   metrics_history['success_rate'],
                   c=metrics_history['episode'],
                   cmap='cool',
                   alpha=0.7,
                   s=50)
axes[1, 0].set_xlabel('√ânergie (J)')
axes[1, 0].set_ylabel('Taux de Succ√®s')
axes[1, 0].set_title('Relation √ânergie-Fiabilit√©')
axes[1, 0].grid(True, alpha=0.3)

# 3.4 Distribution finale
metrics_final = ['R√©compense', '√ânergie', 'Latence', 'Succ√®s', 'D√©chargement']
values_final = [
    metrics_history['total_reward'][-1],
    metrics_history['avg_energy'][-1],
    metrics_history['avg_latency'][-1],
    metrics_history['success_rate'][-1],
    metrics_history['offload_rate'][-1]
]

# Normaliser pour le radar chart
values_normalized = [
    (metrics_history['total_reward'][-1] - min(metrics_history['total_reward'])) / 
    (max(metrics_history['total_reward']) - min(metrics_history['total_reward'])),
    1 - (metrics_history['avg_energy'][-1] - min(metrics_history['avg_energy'])) / 
    (max(metrics_history['avg_energy']) - min(metrics_history['avg_energy'])),
    1 - (metrics_history['avg_latency'][-1] - min(metrics_history['avg_latency'])) / 
    (max(metrics_history['avg_latency']) - min(metrics_history['avg_latency'])),
    metrics_history['success_rate'][-1],
    metrics_history['offload_rate'][-1]
]

bars = axes[1, 1].bar(metrics_final, values_normalized, 
                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'],
                      alpha=0.8)
axes[1, 1].set_ylabel('Valeur Normalis√©e')
axes[1, 1].set_title('Performance Finale (Dernier √âpisode)')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].set_ylim([0, 1])
axes[1, 1].grid(True, alpha=0.3, axis='y')

# Ajouter les valeurs
for bar, value, norm in zip(bars, values_final, values_normalized):
    height = bar.get_height()
    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{norm:.2f}\n({value:.3f})', 
                   ha='center', va='bottom', fontsize=8)

plt.suptitle('Analyse Comparative et Trade-offs DRL IoT-MEC', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('comparative_analysis.png', dpi=300, bbox_inches='tight')
print("  ‚úì comparative_analysis.png")

# 4. RAPPORT FINAL
print("\n" + "="*60)
print("RAPPORT FINAL D'ENTRA√éNEMENT")
print("="*60)

# Calcul des am√©liorations
initial_metrics = {
    'reward': metrics_history['total_reward'][0],
    'energy': metrics_history['avg_energy'][0],
    'latency': metrics_history['avg_latency'][0],
    'success': metrics_history['success_rate'][0],
    'offload': metrics_history['offload_rate'][0]
}

final_metrics = {
    'reward': metrics_history['total_reward'][-1],
    'energy': metrics_history['avg_energy'][-1],
    'latency': metrics_history['avg_latency'][-1],
    'success': metrics_history['success_rate'][-1],
    'offload': metrics_history['offload_rate'][-1]
}

# Calcul des am√©liorations en pourcentage
improvements = {}
for key in initial_metrics.keys():
    if key == 'energy' or key == 'latency':
        # Pour l'√©nergie et la latence, on veut une r√©duction (valeur plus basse)
        improvements[key] = ((initial_metrics[key] - final_metrics[key]) / initial_metrics[key]) * 100
    else:
        # Pour les autres, on veut une augmentation (valeur plus haute)
        improvements[key] = ((final_metrics[key] - initial_metrics[key]) / initial_metrics[key]) * 100

print(f"\nüéØ AM√âLIORATIONS APR√àS {EPISODES} √âPISODES:")
print("-" * 50)
print(f"üìä R√âCOMPENSE:      {initial_metrics['reward']:.4f} ‚Üí {final_metrics['reward']:.4f} ({improvements['reward']:+.1f}%)")
print(f"‚ö° √âNERGIE:         {initial_metrics['energy']:.3f} J ‚Üí {final_metrics['energy']:.3f} J ({improvements['energy']:+.1f}%)")
print(f"‚è±Ô∏è  LATENCE:         {initial_metrics['latency']:.1f} ms ‚Üí {final_metrics['latency']:.1f} ms ({improvements['latency']:+.1f}%)")
print(f"‚úÖ SUCC√àS:          {initial_metrics['success']:.2%} ‚Üí {final_metrics['success']:.2%} ({improvements['success']:+.1f}%)")
print(f"üîÑ D√âCHARGEMENT:    {initial_metrics['offload']:.2%} ‚Üí {final_metrics['offload']:.2%} ({improvements['offload']:+.1f}%)")

# Statistiques suppl√©mentaires
print(f"\nüìà STATISTIQUES SUPPL√âMENTAIRES:")
print("-" * 50)
print(f"  ‚Ä¢ R√©compense moyenne finale: {np.mean(metrics_history['total_reward'][-10:]):.4f}")
print(f"  ‚Ä¢ √ânergie moyenne finale: {np.mean(metrics_history['avg_energy'][-10:]):.3f} J")
print(f"  ‚Ä¢ Latence moyenne finale: {np.mean(metrics_history['avg_latency'][-10:]):.1f} ms")
print(f"  ‚Ä¢ Succ√®s moyen final: {np.mean(metrics_history['success_rate'][-10:]):.2%}")
print(f"  ‚Ä¢ D√©chargement moyen final: {np.mean(metrics_history['offload_rate'][-10:]):.2%}")

# Corr√©lations
print(f"\nüîó CORR√âLATIONS IMPORTANTES:")
print("-" * 50)
correlations = df_metrics[['total_reward', 'avg_energy', 'avg_latency', 'success_rate', 'offload_rate']].corr()
print(f"  ‚Ä¢ R√©compense ‚Üî √ânergie: {correlations.loc['total_reward', 'avg_energy']:.3f}")
print(f"  ‚Ä¢ R√©compense ‚Üî Latence: {correlations.loc['total_reward', 'avg_latency']:.3f}")
print(f"  ‚Ä¢ √ânergie ‚Üî Latence: {correlations.loc['avg_energy', 'avg_latency']:.3f}")
print(f"  ‚Ä¢ D√©chargement ‚Üî Succ√®s: {correlations.loc['offload_rate', 'success_rate']:.3f}")

print("\n" + "="*60)
print("üéâ VISUALISATIONS G√âN√âR√âES AVEC SUCC√àS!")
print("="*60)
print("\nüìÅ FICHIERS CR√â√âS:")
print("  ‚Ä¢ training_metrics.csv - Donn√©es brutes de l'entra√Ænement")
print("  ‚Ä¢ ppo_model.pth - Mod√®le PPO entra√Æn√©")
print("  ‚Ä¢ multi_metrics_analysis.png - 6 m√©triques individuelles")
print("  ‚Ä¢ combined_metrics_evolution.png - Toutes les m√©triques combin√©es")
print("  ‚Ä¢ comparative_analysis.png - Analyses comparatives et trade-offs")

print("\n‚úÖ L'ENTRA√éNEMENT ET L'ANALYSE SONT TERMIN√âS AVEC SUCC√àS!")
print("="*60)

# Afficher les graphiques
plt.show()